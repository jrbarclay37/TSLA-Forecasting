{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investor Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion, we will be using comments scraped from r/wallstreetbets to determine the community's sentiment of TSLA over time. It is important to note that this is an eccentric community with unconventional trading practices and sometimes explicit language in their comments.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import getcwd, listdir\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import ast\n",
    "import pytz\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and append all r/wallstreetbets .csv files scraped from Reddit to dataframe\n",
    "df = pd.DataFrame(columns=['id','post','created_utc','body','score'])\n",
    "for filename in listdir(getcwd()+'\\\\WallStreetBets'):\n",
    "    if filename[-4:] == '.csv':\n",
    "        df = pd.concat([df, pd.read_csv(getcwd()+'\\\\WallStreetBets\\\\'+filename)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351250, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of records\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>la75n9</td>\n",
       "      <td>Daily Discussion Thread #2 for February 1, 2021</td>\n",
       "      <td>1612288210</td>\n",
       "      <td>b' Wish there was a broker option to diamond h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>la75n9</td>\n",
       "      <td>Daily Discussion Thread #2 for February 1, 2021</td>\n",
       "      <td>1612232343</td>\n",
       "      <td>b'Listen up and don\\'t fret all you re-re\\'s. ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>la75n9</td>\n",
       "      <td>Daily Discussion Thread #2 for February 1, 2021</td>\n",
       "      <td>1612229022</td>\n",
       "      <td>b'I am a NOK holder so  not trying to kill the...</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>la75n9</td>\n",
       "      <td>Daily Discussion Thread #2 for February 1, 2021</td>\n",
       "      <td>1612219285</td>\n",
       "      <td>b'If you aren\\xe2\\x80\\x99t buying at discount,...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>la75n9</td>\n",
       "      <td>Daily Discussion Thread #2 for February 1, 2021</td>\n",
       "      <td>1612216647</td>\n",
       "      <td>b'newbies snoozing if u dont buy a tsla dip'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             post created_utc  \\\n",
       "0  la75n9  Daily Discussion Thread #2 for February 1, 2021  1612288210   \n",
       "1  la75n9  Daily Discussion Thread #2 for February 1, 2021  1612232343   \n",
       "2  la75n9  Daily Discussion Thread #2 for February 1, 2021  1612229022   \n",
       "3  la75n9  Daily Discussion Thread #2 for February 1, 2021  1612219285   \n",
       "4  la75n9  Daily Discussion Thread #2 for February 1, 2021  1612216647   \n",
       "\n",
       "                                                body score  \n",
       "0  b' Wish there was a broker option to diamond h...     1  \n",
       "1  b'Listen up and don\\'t fret all you re-re\\'s. ...    13  \n",
       "2  b'I am a NOK holder so  not trying to kill the...    -9  \n",
       "3  b'If you aren\\xe2\\x80\\x99t buying at discount,...    21  \n",
       "4       b'newbies snoozing if u dont buy a tsla dip'     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we will do when processsing the data, is convert emojis to text. There can be a lot of meaning in emojis and we don't want to omit this information. Below is an example of an extremely bullish tweet with a lot of emojis used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOL üåàüêªs capitulated. Never bet against Papa Musk. TSLA to the moon!!! üåïüöÄ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode emoji example \n",
    "comment = \"LOL \\\\xF0\\\\x9F\\\\x8C\\\\x88\\\\xF0\\\\x9F\\\\x90\\\\xBBs capitulated. Never bet against Papa Musk. TSLA to the moon!!! \\\\xF0\\\\x9F\\\\x8C\\\\x95\\\\xf0\\\\x9f\\\\x9a\\\\x80\"\n",
    "comment = \"b'\"+comment+\"'\"\n",
    "comment = ast.literal_eval(comment)\n",
    "comment.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the particular example above, we could probably determine the sentiment without the emojis, but this may not always be the case. Below, we see an example with TSLA, a bull emoji and the word \"rekt\". Now without this emoji and the knowledge that this user intentionally mispelled the word \"wrecked\", this would be extremely difficult to interpret. It would be difficult to account for all of the mispelled words and slang in this group. However, usually bull and bear emojis are used to taunt people on the losing end. So in this particular case, we could assume this is actually a bearish post taunting the TSLA bulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TSLA üêÇs rekt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode emoji example \n",
    "comment = \"TSLA \\\\xF0\\\\x9F\\\\x90\\\\x82s rekt\"\n",
    "comment = \"b'\"+comment+\"'\"\n",
    "comment = ast.literal_eval(comment)\n",
    "comment.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a dictionary to map all emojis to their respective names with the help of https://apps.timwhitlock.info/emoji/tables/unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to replace emoji bytes with words\n",
    "emoji_df = pd.read_excel(getcwd()+'\\\\emoji_unicode.xlsx', encoding='ISO-8859-1', sheet_name='emoji_short')\n",
    "emoji_df[\"Bytes\"] = emoji_df[\"Bytes\"].apply(lambda x: x.lower())\n",
    "emoji_df[\"Description\"] = emoji_df[\"Description\"].apply(lambda x: \" \" + x + \" \") #pad word substitute\n",
    "emoji_dict = dict(zip(emoji_df['Bytes'], emoji_df['Description']))\n",
    "\n",
    "# Loop through each post and replace emojis\n",
    "for i in range(df.shape[0]):\n",
    "    for k, v in zip(emoji_dict.keys(), emoji_dict.values()):\n",
    "        if re.search(rf\"{re.escape(k)}\", df.loc[i, 'body']):\n",
    "            df.loc[i,'body'] = re.sub(rf\"{re.escape(k)}\", v, df.loc[i, 'body'])\n",
    "            \n",
    "# remove unconverted emojis and unknown characters from message\n",
    "df['body'] = df['body'].apply(lambda x: ast.literal_eval(x))\n",
    "df['body'] = df['body'].apply(lambda x: x.decode('utf-8'))\n",
    "printable = set(string.printable)\n",
    "df['body'] = df['body'].apply(lambda x: ''.join(filter(lambda y: y in printable, x)).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the comment timestamps from UTC -> EST because our data from the stock market is in EST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert epoch to UTC\n",
    "df['created_utc'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "# Convert UTC -> EST\n",
    "df['created_utc'] = df['created_utc'].dt.tz_localize('UTC')\n",
    "df['created_utc'] = df['created_utc'].dt.tz_convert('US/Eastern')\n",
    "df['created_utc'] = df['created_utc'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df['created_utc'] = df['created_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint - Load Processed Posts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the previous steps can take a few minutes to run, so the cell below allows us to pick up where we left off by reading in the preprocessed posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file for checkpoint\n",
    "#df.to_csv('Preprocessed_WSB_Posts.csv',index=False)\n",
    "# Read file\n",
    "df = pd.read_csv('.//Preprocessed_WSB_Posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more preprocessing steps we need to take care of before our data is ready for sentiment analysis. This includes removing unhelpful characters, extracting call and put options, which are bullish/bearish indicators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "df['body'] = df['body'].str.lower()\n",
    "df['body'] = df['body'].apply(lambda x: ''.join(u for u in x if u not in ('?','.',';',':','!','\"',',','(',')')))\n",
    "\n",
    "# Flag call/put options (i.e. Replace \"$1000c\" with \"call\")\n",
    "df['option'] = df['body'].apply(lambda x: re.findall(r'(\\$?[0-9]+(p|c))', x))\n",
    "df['option'] = df['option'].apply(lambda x: [y[1] for y in x])\n",
    "df['call'] = df['option'].apply(lambda x: 1 if 'c' in x else 0)\n",
    "df['put'] = df['option'].apply(lambda x: 1 if 'p' in x else 0)\n",
    "df['body'] = df['body'].apply(lambda x: re.sub(r'(\\$?[0-9]+(p|c))', '', x))\n",
    "\n",
    "# Remove dates and numbers and other characters\n",
    "df['body'] = df['body'].apply(lambda x: ''.join(i for i in x if i not in ('$','/','%')))\n",
    "df['body'] = df['body'].apply(lambda x: x.replace('&amp', ''))\n",
    "df['body'] = df['body'].apply(lambda x: ''.join(i for i in x if not i.isdigit()))\n",
    "\n",
    "# add call and put positions detected in posts\n",
    "df['body'] = df.apply(lambda row: row['body'] + ' call' if row['call'] > 0 else row['body'], axis=1)\n",
    "df['body'] = df.apply(lambda row: row['body'] + ' put' if row['put'] > 0 else row['body'], axis=1)\n",
    "\n",
    "# Trim all whitespace\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Lemmatization and removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will use a natural language processing technique, called tokenization to split each post into a list of words, which we will call tokens. We remove stop words that do not add value, such as \"I\", \"you\", \"hereafter\", \"thus\", \"indeed\", \"whereupon\"... You get the point. Additionally, we will use another method, called lemmatization to normalize words derived from the same word, but used in different inflected forms (i.e. code, codes, coding, coded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization, Lemmatization and removing stopwords\n",
    "#nlp = en_core_web_sm.load() \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stop = set(stopwords.words('english'))\n",
    "stop = nlp.Defaults.stop_words\n",
    "\n",
    "# remove relevant words from stop list\n",
    "keep_words = ['call','put','up','down']\n",
    "for w in keep_words:\n",
    "    stop.remove(w)\n",
    "    \n",
    "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "def process_comments(text):\n",
    "    \"\"\"Convert user comments to tokenized strings\"\"\"\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "        if i.lower() not in stop:\n",
    "            word = lemmatizer.lemmatize(i)\n",
    "            final_text.append(word.lower())\n",
    "    return ' '.join(final_text)\n",
    "\n",
    "df['tokenized'] = df['body'].apply(process_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Jaccard Similarity Scores to label posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our posts are not already labeled and we do not have time to manually label 280,000+ posts, we will start with two word banks with words we already know indicate bullish/bearish sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of words\n",
    "bullish_words = '''call up moon moonin rocket long pump pumpin bear rainbow against doubt rally'''\n",
    "\n",
    "bearish_words = '''put down drop drill drillin dump dumpin short ox bull rug'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our two word banks, we will create sentiment scores using the Jaccard Similarity Index. This approach will measure the similarity between each user comment and the word bank, comparing the members in each set to come up with a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity Scores\n",
    "def jaccard_similarity(group, comment):\n",
    "    intersection = set(group).intersection(set(comment))\n",
    "    union = set(group).union(set(comment))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def get_scores(group, comments):\n",
    "    scores = []\n",
    "    for c in comments:\n",
    "        s = jaccard_similarity(group, c)\n",
    "        scores.append(s)\n",
    "    return scores\n",
    "\n",
    "bull_scores = get_scores(bullish_words, list(df['tokenized']))\n",
    "bear_scores = get_scores(bearish_words, list(df['tokenized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scores\n",
    "jdf = df.copy()\n",
    "jdf['Bullish'] = bull_scores\n",
    "jdf['Bearish'] = bear_scores\n",
    "jdf = jdf[['created_utc','tokenized','score','Bullish','Bearish']]\n",
    "jdf = jdf.rename(columns={'created_utc':'Created_EST','tokenized':'Comment','score':'Score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tesla up sub doesnt notice care man place different couple week ago'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdf.loc[15,'Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our jaccard similarity scores, we will flag each comment as bullish with a value of \"1\" or bearish with a value of \"-1\" depending on whose score is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Sentiment Scores\n",
    "jdf['Sentiment'] = jdf.apply(lambda row: 1 if row[\"Bullish\"] >= row[\"Bearish\"] else -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dataframes for two Naive Bayes Classifiers to predict bullish/bearish posts\n",
    "bull_df = jdf[['Comment','Sentiment']].copy()\n",
    "bear_df = jdf[['Comment','Sentiment']].copy()\n",
    "\n",
    "# Bullish flag\n",
    "bull_df['Sentiment'] = bull_df['Sentiment'].apply(lambda x: 0 if x == -1 else x)\n",
    "\n",
    "# Bearish flag\n",
    "bear_df['Sentiment'] = bear_df['Sentiment'].apply(lambda x: 0 if x == 1 else x)\n",
    "bear_df['Sentiment'] = bear_df['Sentiment'].apply(lambda x: 1 if x == -1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try to enhance our approach by using a Naive Bayes classifier to identify other words indicative of bullish/bearish sentiment that we can add to our word banks. To do this, we will need to vectorize our set of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(stop_words='english') # don't use\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(jdf['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print vectors\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351250, 75064)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokens vectorized (bag of words), we can fit a Naive Bayes classifier to the data. We will fit two separate models for bullish and bearish sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullish classifier\n",
    "bull_clf = MultinomialNB(alpha=.01).fit(vectors, bull_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the most important words for classifying sentiment by using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_words(classifier, vectorizer, categories, top_n):\n",
    "    \"\"\"Function to show most important words from NB classifier\"\"\"\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top = np.argsort(classifier.coef_[i])[-top_n:]\n",
    "        print(f\"{category}: {' '.join(feature_names[top])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bullish: month red probably spread high drop run thing end yesterday bull go gain morning fucked long papa lmao battery feel pump new car come big profit selling retard apple friday don eow split option hit company nio play hold holding close bear look green year want guy way earnings let short people aapl moon it belong spy need price dont know good right eod sold got shit face dip market open share fucking buying musk think time lol rocket gonna sell money im bought down stock week tomorrow going fuck today day like buy up put elon call tesla tsla\n"
     ]
    }
   ],
   "source": [
    "show_top_words(bull_clf, vectorizer, [\"Bullish\"], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearish classifier\n",
    "bear_clf = MultinomialNB(alpha=.01).fit(vectors, bear_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearish: that don imo gain run ath elons fomo stop tweet shop people diamond green printing rn imagine go nah let bull split big wish it expiring eod guh dumping option strength god belongs print want mooning wtf drop thats amzn long power earnings damn holder dont open retard need lmao rip bullish dump nio time sell overnight aapl who got worth good moon hope sold pump morning oh hard shorting hand month high belong im whats thought share lol call thing ah dip shit hour hit gonna hold down short right going holding up tomorrow elon bought put tesla tsla\n"
     ]
    }
   ],
   "source": [
    "show_top_words(bear_clf, vectorizer, [\"Bearish\"], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Word Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of words\n",
    "bullish_words = '''call up moon moonin mooning rocket long pump pumpin bear rainbow against doubt long\n",
    "                    green rally strength papa battery'''\n",
    "\n",
    "bearish_words = '''put down drop drill drillin dump dumpin short shorting ox bull bulls rug rip fomo red'''\n",
    "\n",
    "# Jaccard Similarity Scores\n",
    "def jaccard_similarity(group, comment):\n",
    "    intersection = set(group).intersection(set(comment))\n",
    "    union = set(group).union(set(comment))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def get_scores(group, comments):\n",
    "    scores = []\n",
    "    for c in comments:\n",
    "        s = jaccard_similarity(group, c)\n",
    "        scores.append(s)\n",
    "    return scores\n",
    "\n",
    "bull_scores = get_scores(bullish_words, list(df['tokenized']))\n",
    "bear_scores = get_scores(bearish_words, list(df['tokenized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf = df.copy()\n",
    "jdf['Bullish'] = bull_scores\n",
    "jdf['Bearish'] = bear_scores\n",
    "\n",
    "jdf = jdf[['created_utc','tokenized','score','Bullish','Bearish']]\n",
    "jdf = jdf.rename(columns={'created_utc':'Created_EST','tokenized':'Comment','score':'Score'})\n",
    "jdf.head()\n",
    "jdf['Sentiment'] = jdf.apply(lambda row: 1 if row[\"Bullish\"] >= row[\"Bearish\"] else -1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to create sentiment scores entirely based on a word bank, and scoring based on the ratio of bullish/bearish words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create bull and bear sentiment scores for each day using keywords\n",
    "bullish_words = ['call', 'up', 'moon', 'moonin', 'mooning', 'rocket', 'long', 'pump', 'pumpin', 'bear', 'rainbow',\n",
    "                 'against', 'doubt', 'long', 'green', 'rally', 'strength', 'papa', 'battery']\n",
    "\n",
    "bearish_words = ['put', 'down', 'drop', 'drill', 'drillin', 'dump', 'dumpin', 'short', 'shorting',\n",
    "                 'ox', 'bull', 'bulls', 'rug', 'rip', 'fomo', 'red']\n",
    "\n",
    "jdf['Bull Count'] = jdf['Comment'].apply(lambda x: len([w for w in x.split(' ') if w in bullish_words]))\n",
    "jdf['Bear Count'] = jdf['Comment'].apply(lambda x: len([w for w in x.split(' ') if w in bearish_words]))\n",
    "\n",
    "jdf['Sentiment WB'] = jdf.apply(lambda row: (row[\"Bull Count\"] - row[\"Bear Count\"])/max((row[\"Bull Count\"] + row[\"Bear Count\"]),1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Sentiment Analysis (Valence Aware Dictionary for sEntiment Reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish, we will create one more sentiment variable using \"VADER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.nltk.org/howto/sentiment.html\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER is a parsimonious rule-based model developed by a group of Georgia Tech researchers for sentiment analysis of social media text. You can view their research paper here: (http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "#sid.polarity_scores(x) #{'neg': 0.735, 'neu': 0.265, 'pos': 0.0, 'compound': -0.7616}\n",
    "jdf['VADER Score'] = jdf['Comment'].apply(lambda x: sid.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create weighted scores using the number of upvotes and downvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights based on upvotes/downvotes\n",
    "jdf.loc[jdf['Score'] == 0,'Score'] = 1\n",
    "jdf['Weighted Sentiment'] = jdf['Sentiment']*jdf['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to date based on market close - If >= 16:00 (4pm) then +1 day\n",
    "#jdf['Created_EST'] = jdf['Created_EST'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "jdf['Post Hour'] = jdf['Created_EST'].apply(lambda x: x.strftime('%H'))\n",
    "jdf['Created_EST'] = jdf.apply(lambda row: row['Created_EST'] + timedelta(days=1) if int(row['Post Hour']) >= 16 else row['Created_EST'], axis=1)\n",
    "jdf['Created_EST'] = jdf['Created_EST'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "agg_df = jdf.groupby('Created_EST').apply(lambda x: pd.Series({\n",
    "            'weighted_avg': x['Weighted Sentiment'].mean(), # upvote weighted score\n",
    "            'avg': x['Sentiment'].mean(),    # original sentiment score\n",
    "            'count': x['Sentiment'].count(), # comment volume\n",
    "            'wb': x['Sentiment WB'].mean(),  # word bank sentiment score\n",
    "            'vader': x['VADER Score'].mean() # vader sentiment score\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_avg</th>\n",
       "      <th>avg</th>\n",
       "      <th>count</th>\n",
       "      <th>wb</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Created_EST</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>1.241135</td>\n",
       "      <td>0.691095</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.046129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>2.695002</td>\n",
       "      <td>0.709946</td>\n",
       "      <td>4082.0</td>\n",
       "      <td>0.161087</td>\n",
       "      <td>0.074849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>3.214615</td>\n",
       "      <td>0.708621</td>\n",
       "      <td>4338.0</td>\n",
       "      <td>0.236101</td>\n",
       "      <td>0.045215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>2.468863</td>\n",
       "      <td>0.603455</td>\n",
       "      <td>4978.0</td>\n",
       "      <td>0.186348</td>\n",
       "      <td>0.053659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>1.326475</td>\n",
       "      <td>0.699393</td>\n",
       "      <td>5103.0</td>\n",
       "      <td>0.145415</td>\n",
       "      <td>0.061487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             weighted_avg       avg   count        wb     vader\n",
       "Created_EST                                                    \n",
       "2020-02-05       1.241135  0.691095  3807.0  0.062000  0.046129\n",
       "2020-08-20       2.695002  0.709946  4082.0  0.161087  0.074849\n",
       "2020-09-01       3.214615  0.708621  4338.0  0.236101  0.045215\n",
       "2020-07-22       2.468863  0.603455  4978.0  0.186348  0.053659\n",
       "2020-02-04       1.326475  0.699393  5103.0  0.145415  0.061487"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df = agg_df.sort_values(by=['count'])\n",
    "agg_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all wrapped up with the sentiment scores. Scores greater than 0 are bullish and less than zero are bearish. We can output this file and use it in our forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "agg_df.to_csv('Sentiment Scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
