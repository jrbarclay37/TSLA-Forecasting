{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping r/wallstreetbets from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use python's API wrapper for Reddit, PRAW, to scrape comments from r/wallstreetbets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import praw\n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "import http.client\n",
    "import pandas as pd\n",
    "from os import getcwd\n",
    "\n",
    "# paths and keys\n",
    "file_dir = getcwd()\n",
    "client_id = 'INSERT CLIENT ID'\n",
    "user_agent = 'INSERT USER AGENT'\n",
    "secret = 'INSERT SECRET KEY'\n",
    "redirect_uri = 'http://localhost:8080'\n",
    "\n",
    "# Initialize reddit connection\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=secret, user_agent=user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to compile all of the posts we are interested in collecting comments from. We want posts that either include \"Daily Discussion Thread\", \"Moves Tomorrow\", or \"Weekend Discussion Thread\". These are all headings used regularly each day/weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe of WSB posts\n",
    "wsb = reddit.subreddit('wallstreetbets')\n",
    "week_posts = []\n",
    "for post in wsb.search('Daily Discussion Thread', limit=400):\n",
    "    week_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "week_posts = pd.DataFrame(week_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "\n",
    "tomorrow_posts = []\n",
    "for post in wsb.search('Moves Tomorrow', limit=400):\n",
    "    tomorrow_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "tomorrow_posts = pd.DataFrame(tomorrow_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "\n",
    "weekend_posts = []\n",
    "for post in wsb.search('Weekend Discussion Thread', limit=400):\n",
    "    weekend_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "weekend_posts = pd.DataFrame(weekend_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "\n",
    "wsb_posts = pd.concat([week_posts, tomorrow_posts, weekend_posts], ignore_index=True)\n",
    "\n",
    "#Filter for only relevant wsb posts\n",
    "wsb_posts['flag'] = wsb_posts['title'].apply(lambda x: 'Y' if any(w in x.upper() for w in ['DAILY DISCUSSION','WEEKEND DISCUSSION','MOVES TOMORROW']) else 'N')\n",
    "wsb_posts = wsb_posts[wsb_posts['flag'] == 'Y'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRAW is great for scraping post titles. However, it is incredibly slow when scraping posts with high volumes of comments. Because of this, we are going to query the pushshift API. This is a project warehouses all data from Reddit, allowing us to query the data more efficiently using our post id's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scraping all posts, we will use the code below to omit any posts we have already stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of already scraped posts\n",
    "warehoused_posts = []\n",
    "for filename in os.listdir(os.getcwd()+'\\\\Sentiment Analysis\\\\WallStreetBets'):\n",
    "    if filename[-4:] == '.csv':\n",
    "        warehoused_posts.append(filename[:-4])\n",
    "\n",
    "wsb_posts = wsb_posts[~wsb_posts['title'].isin(warehoused_posts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to query the pushshift API and store our data in flat files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through posts in r/wallstreetbets and warehouse data\n",
    "count = 0\n",
    "for post_id, post_name in zip(wsb_posts['id'], wsb_posts['title']):\n",
    "    \n",
    "    attempts = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if attempts > 2:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Query comments\n",
    "            url = 'https://api.pushshift.io/reddit/comment/search/?link_id='+post_id+'&limit=100000'\n",
    "            r = requests.get(url)\n",
    "            data = json.loads(r.text)\n",
    "            wsb_comments = data['data']\n",
    "            wsb_df = pd.DataFrame(wsb_comments)\n",
    "            wsb_df = wsb_df[['body','created_utc','score']]\n",
    "\n",
    "            # Filter for TSLA comments\n",
    "            key_words = ['TSLA','TESLA','MUSK','ELON']\n",
    "            wsb_df['flag'] = wsb_df['body'].apply(lambda x: 'Y' if any(w in x.upper() for w in key_words) else 'N')\n",
    "            wsb_df = wsb_df[wsb_df['flag'] == 'Y']\n",
    "\n",
    "            wsb_df['id'] = post_id\n",
    "            wsb_df['post'] = post_name\n",
    "            wsb_df['body'] = wsb_df['body'].apply(lambda x: x.encode('utf-8'))\n",
    "\n",
    "            wsb_df = wsb_df[['id','post','created_utc','body','score']]\n",
    "            wsb_df.to_csv(file_dir+'\\\\Sentiment Analysis\\\\WallStreetBets\\\\'+post_name+'.csv',index=False)\n",
    "            print(f\"Successfully scraped - {post_name}\")\n",
    "            break\n",
    "        except:\n",
    "            print(f\"Error encountered for {post_name}... restarting iteration {count}\")\n",
    "            attempts += 1\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    \n",
    "    count +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
